---
title: "Final Project - GIS"
author: "Thomas Beier, Florian Franz, Konstantin Seeger"
date: "09.02.2022"
output: pdf_document
---

## Spatial prediction of forest microclimate using LiDAR data

Using the `lidr` package by J.-R. Roussel for processing LiDAR data and creating a forest microclimate model.

For information see the book (https://r-lidar.github.io/lidRbook/index.html), the package documentation (https://cran.r-project.org/web/packages/lidR/index.html) and also this publication (https://www.sciencedirect.com/science/article/pii/S0034425720304314).



```{r, results='hide', warning=FALSE, message=FALSE, echo=FALSE}
### Set up a working environment

require(envimaR)

packagesToLoad = c("mapview", "raster", "sf", "caret", "exactextractr",  
                   "doParallel", "CAST", "ranger","rasterVis","dplyr",
                   "lidR", "RColorBrewer")

# Define a project rootfolder
# This is the mandantory rootfolder of the whole project, "E:/" has to be changed to the respective directory where the edu folder is located 
rootDir = "E:/edu/GIS_Project_Beier_Franz_Seeger"  

projectDirList   = c("data/", # data folder
                     "data/raw/",
                     "data/processed/", # folder for processed data (.rds)
                     "data/predictors/",
                     "data/modelling/",
                     "data/modelling/model_training_data/",
                     "data/modelling/models/",
                     "data/modelling/prediction/",
                     "data/modelling/validation/",
                     "docs/",
                     "tmp/",
                     "src/",
                     "src/functions/")

# Now set automatically root direcory, folder structure and load libraries
envrmt = envimaR::createEnvi(root_folder = rootDir,
                             folders = projectDirList,
                             path_prefix = "path_",
                             libs = packagesToLoad,
                             alt_env_id = "COMPUTERNAME",
                             alt_env_value = "PCRZP",
                             alt_env_root_folder = "F:/BEN/edu")
## set raster temp path
raster::rasterOptions(tmpdir = envrmt$path_tmp)
```

### Read las file

```{r, warning=FALSE, message=FALSE}
# The las file must be in the folder data/raw!!!
las_files <- list.files(envrmt$path_raw,
                       pattern = glob2rx("*.las"),
                       full.names = TRUE)

las <- readLAS(las_files[1])

# Assign a coord. ref. syst. (CRS)
# In this case UTM zone 32N
epsg_number <- 25832
crs(las) <- epsg_number

las

# Check las file
las_check(las)
```

```{r}
### Some plotting...

# Basic 3D plot
#plot(las)

# Example cross section 2D plot (along a transect)
p1 <- c(477500, 5632500) # these are coordinates
p2 <- c(478217.5, 5632500 ) # these are coordinates
las_tr <- clip_transect(las, p1, p2, width = 4, xz = TRUE)

ggplot(las_tr@data, aes(X,Z, color = Z)) + 
  geom_point(size = 0.5) + 
  coord_equal() + 
  theme_minimal() +
  scale_color_gradientn(colours = height.colors(25))
```

### Predictor set

List of variables used for forest microclimate prediction:

- Canopy height (CHM)
- Standard metrics (mean, max, sd...)
- Mean height of first returns
- Maxiumum height of first returns
- Standard deviation of first returns
- Point density
- Pulse density
- Leaf area index (LAI)
- Elevation (DTM)
- Slope
- Exposition
- Topographic position index (TPI)
- Irradiance

All of the predictors are calculated at the grid level within 1 m  x 1 m pixels.

#### Canopy height model (CHM)

```{r, warning=FALSE, message=FALSE}
# Height normalization within the point cloud using invert distance weighting
norm_las <- normalize_height(las, knnidw())

# Check if all ground points are 0
hist(filter_ground(norm_las)$Z, 
     breaks = seq(-0.45, 0.45, 0.01),
     main = "", xlab = "Elevation")

if (!file.exists(paste0(envrmt$path_predictors, "/chm.RDS"))) {
# Calculate CHM using pit-free algorithm (we can discuss about this --> which algorithm we want to use)
  chm <- grid_canopy(norm_las, res = 1.0, 
                    pitfree(thresholds = c(0, 2, 5, 10, 15),
                            max_edge = c(0, 1.5)))

  saveRDS(chm, paste0(envrmt$path_predictors, "/chm.RDS"))
  
} else {
  
  chm <- readRDS(paste0(envrmt$path_predictors, "/chm.RDS"))

}
  
chm # min value of -0.003 and max value of 42.168...maybe false
```

```{r, echo=FALSE}
# tmap plot
library(tmap)
tm_shape(chm) +
tm_raster(title = "Pitfree CHM 1 m² cells", palette = height.colors(20)) +
  tm_grid() +
  tm_layout(legend.outside = TRUE)
```

#### Standard metrics of the canopy

```{r}
if (!file.exists(paste0(envrmt$path_predictors, "/stdmetrics.RDS"))) {

  chm_stdmetrics <- grid_metrics(norm_las, .stdmetrics, res = 1.0)
  
  saveRDS(chm_stdmetrics, paste0(envrmt$path_predictors, "/stdmetrics.RDS"))

} else {
  
  chm_stdmetrics <- readRDS(paste0(envrmt$path_predictors, "/stdmetrics.RDS"))

}

plot(chm_stdmetrics, col = height.colors(20))
```

#### First returns mean height, maxiumum height and standard deviation of the height from the first returns

```{r}
# Function for calculating mean, max and standard deviation of first returns
first_return_metrics <- function(x) {
  list(mean = mean(x),
       max = max(x), 
       sd(x))
}


if (!file.exists(paste0(envrmt$path_predictors, "/first_returns.RDS"))) {

  return_mean_max_sd <- grid_metrics(norm_las, func = ~first_return_metrics(Z),
                                   res = 1.0, filter = ~ReturnNumber == 1L)
  
  saveRDS(return_mean_max_sd, paste0(envrmt$path_predictors, "/first_returns.RDS"))

} else {
  
  return_mean_max_sd <- readRDS(paste0(envrmt$path_predictors, "/first_returns.RDS"))

}
```

```{r, echo=FALSE, warning=FALSE, fig.width=12, fig.height=7}
par_org <- par()
par(mfrow = c(2,3), cex.main = 1)

hist(return_mean_max_sd[[1]],
     main = "Histogram of first returns mean height",
     xlab = "Height [m]")

hist(return_mean_max_sd[[2]],
     main = "Histogram of first returns maximum height",
     xlab = "Height [m]")

hist(return_mean_max_sd[[3]],
     main = "Histogram of first returns height standard deviation",
     xlab = "Height [m]")

raster::plot(return_mean_max_sd[[1]],
             col = height.colors(20),
             main = "First returns mean height 1 m² cells")

raster::plot(return_mean_max_sd[[2]],
             col = height.colors(20),
             main = "First returns maximum height 1 m² cells")

col_pal <- colorRampPalette(c("forestgreen","green","yellow","red","purple4"))
raster::plot(return_mean_max_sd[[3]],
             col =  col_pal(10),
             main = "First returns height standard deviation 1 m² cells")

par(par_org)
```

#### Point and pulse density

```{r}
if (!file.exists(paste0(envrmt$path_predictors, "/point_density.RDS"))) {

  point_density <- grid_metrics(norm_las, ~length(Z)/1, res = 1.0)
  pulse_density <- grid_metrics(norm_las, ~length(Z)/1, res = 1.0,
                                filter = ~ReturnNumber == 1L)
  
  saveRDS(point_density, paste0(envrmt$path_predictors, "/point_density.RDS"))
  saveRDS(pulse_density, paste0(envrmt$path_predictors, "/pulse_density.RDS"))

} else {
  
  point_density <- readRDS(paste0(envrmt$path_predictors, "/point_density.RDS"))
  pulse_density <- readRDS(paste0(envrmt$path_predictors, "/pulse_density.RDS"))

}
```

```{r, echo=FALSE, warning=FALSE, fig.width=10, fig.height=7}
par(mfrow = c(2,2), cex.main = 1)

hist(point_density,
     main = "Histogram of point density",
     xlab = "Number of points")

hist(pulse_density,
     main = "Histogram of pulse density",
     xlab = "Number of points")

raster::plot(point_density, 
             col = gray.colors(50,0,1),
             main = "Point density 1 m² cells")

raster::plot(pulse_density, 
             col = gray.colors(50,0,1),
             main = "Pulse density 1 m² cells")

par(par_org)
```

#### Leaf area index (LAI)

```{r}
# Calculate Leaf area index with the "canopyLazR" package. Installation can be found on the following github page: https://github.com/akamoske/canopyLazR
# Convert .laz or .las file into a voxelized lidar array
library(canopyLazR)

if (!file.exists(paste0(envrmt$path_predictors, "/lai.tif"))) {

  laz_data <- laz.to.array(laz.file.path = file.path(envrmt$path_raw,"las_mof.las"), 
                          voxel.resolution = 1, 
                          z.resolution = 1,
                          use.classified.returns = TRUE)

  # Level the voxelized array to mimic a canopy height model
  level_canopy <- canopy.height.levelr(lidar.array = laz_data)

  # Estimate Leaf Area Density (LAD) for each voxel in leveled array
  lad_estimates <- machorn.lad(leveld.lidar.array = level_canopy, 
                              voxel.height = 1, 
                              beer.lambert.constant = NULL)

  # Convert the LAD array into a single raster stack
  lad_raster <- lad.array.to.raster.stack(lad.array = lad_estimates, 
                                          laz.array = laz_data, 
                                          epsg.code = 25832) # The epsg code maybe needs to be changed. 


  # Create a single LAI raster from the LAD raster stack
  lai_raster <- raster::calc(lad_raster, fun = sum, na.rm = TRUE)


  
  saveRDS(lai_raster, paste0(envrmt$path_predictors, "/lai.tif"))

} else {
  
  lai_raster <- raster(paste0(envrmt$path_predictors, "/lai.tif"))

}

plot(lai_raster)
```

#### Elevation

```{r, warning=FALSE}
# Calculate DTM using invert distance weighting
if (!file.exists(paste0(envrmt$path_predictors, "/dtm.RDS"))) {

  dtm <- grid_terrain(las, res = 1.0, algorithm = knnidw(k = 6L, p = 2))
  
  saveRDS(dtm, paste0(envrmt$path_predictors, "/dtm.RDS"))

} else {
  
  dtm <- readRDS(paste0(envrmt$path_predictors, "/dtm.RDS"))

}
```

```{r, echo=FALSE, warning=FALSE, fig.width=10, fig.height=5}
par(mfrow = c(1,2), cex.main = 1)

hist(dtm,
     breaks = 20,
     main = "Histogram of the elevation",
     xlab = "Elevation [m]")

lidR::plot(dtm,
           col = height.colors(50),
           main = "Invert distance weighting DTM 1 m² cells")

par(par_org)
```

#### Slope, exposition and TPI

```{r}
if (!file.exists(paste0(envrmt$path_predictors, "/slope.RDS"))) {

  slope <- raster::terrain(dtm, opt = "slope", unit = "degrees", neighbors = 8)
  exposition <- raster::terrain(dtm, opt = "aspect", unit = "degrees", neighbors = 8)
  tpi <- raster::terrain(dtm, opt = "tpi")
  
  saveRDS(slope, paste0(envrmt$path_predictors, "/slope.RDS"))
  saveRDS(exposition, paste0(envrmt$path_predictors, "/exposition.RDS"))
  saveRDS(tpi, paste0(envrmt$path_predictors, "/tpi.RDS"))

} else {
  
  slope <- readRDS(paste0(envrmt$path_predictors, "/slope.RDS"))
  exposition <- readRDS(paste0(envrmt$path_predictors, "/exposition.RDS"))
  tpi <- readRDS(paste0(envrmt$path_predictors, "/tpi.RDS"))

}
```

```{r, echo=FALSE, warning=FALSE, fig.width=10, fig.height=7}
par(mfrow = c(2,3), cex.main = 1)

hist(slope,
     main = "Histogram of slope",
     xlab = "Slope [°]")

hist(exposition,
     main = "Histogram of exposition",
     xlab = "Exposition [°]")

hist(tpi,
     main = "Histogram of tpi")

raster::plot(slope,
             col = topo.colors(25),
             main = "Slope in degrees")

raster::plot(exposition,
             col = bpy.colors(25),
             main = "Exposition in degrees")

raster::plot(tpi,
             col = topo.colors(25),
             main = "TPI")

par(par_org)
```

#### Irradiance

```{r}
### NOT WORKING YET!
```

### Create final predictor stack with temperature (from the climate station data) as response variable

```{r}
# Create a raster stack of all predictors
if (!file.exists(paste0(envrmt$path_predictors, "/predictor_stack.RDS"))) {
  
  predictors <- raster::stack(chm, chm_stdmetrics, return_mean_max_sd,
                              point_density, pulse_density, lai_raster,
                              dtm, slope, exposition, tpi)
  
  # Rename layer names --> only where it's necessary
  predictors@layers[[1]]@data@names <- "chm"
  predictors@layers[[58]]@data@names <- "return_mean"
  predictors@layers[[59]]@data@names <- "return_max"
  predictors@layers[[60]]@data@names <- "return_sd"
  predictors@layers[[61]]@data@names <- "point_density"
  predictors@layers[[62]]@data@names <- "pulse_density"
  predictors@layers[[63]]@data@names <- "lai"
  predictors@layers[[64]]@data@names <- "dtm" # --> has to be changed back to 64 if LAI is included
  
  saveRDS(predictors, file.path(envrmt$path_predictors, "/predictor_stack.RDS"))
  
} else {
  
  predictors <- readRDS(paste0(envrmt$path_predictors, "/predictor_stack.RDS"))
  
}

if (!file.exists(paste0(envrmt$path_processed, "/df_predictors_response.RDS"))) {

  # Read climate data and select timespan (June, July, August)
  climate_data <- readRDS(paste0(envrmt$path_raw, "/climate_stations_combined.RDS"))
  climate_data <- climate_data[order(climate_data$date),]
  climate_data <- climate_data %>% 
    filter(date >= "2020-06-01 00:00:00" & date <= "2020-08-31 23:00:00") 

  # Read shapefile core study trees
  trees <- sf::read_sf(paste0(envrmt$path_raw, "/core_study_trees.shp"))

  # Merge climate data and trees based on the tree_id
  climate_data <- climate_data %>% rename(tree_id = cst_id)
  trees_climate_merged <- merge(climate_data, trees, by = "tree_id")
  trees_climate_merged <- trees_climate_merged[order(trees_climate_merged$date),]

  # Create one final DataFrame with the predictor values for each station
  df_final <- data.frame(matrix(ncol = 67, nrow =  nrow(trees_climate_merged))) # change ncol to 67 if LAI is included
  colnames(df_final) <- names(predictors)

  for (i in 1:67) { # change to 67 if LAI is included
  
    df_final[i] <- raster::extract(predictors[[i]], sf::st_as_sf(trees_climate_merged))
    
    }
  
  # Add temperature and the tree_id
  df_final$temp <- trees_climate_merged$temp
  df_final$tree_id <- trees_climate_merged$tree_id
  df_final$date <- trees_climate_merged$date
  
  # Save as RDS
  saveRDS(df_final, file.path(envrmt$path_processed, "/df_predictors_response.RDS"))
  
} else {
  
  df_final <- readRDS(paste0(envrmt$path_processed, "/df_predictors_response.RDS"))
  
}

# Show amount of data for each station
table(df_final$tree_id)
```

```{r}
# Klimastation hinzufuegen
clim_stat <- readRDS(paste0(envrmt$path_raw, "/klimastation_wiese_hourly.RDS"))
clim_stat <- clim_stat[,-c(14,24)]
df_final_merged <- merge(df_final, clim_stat, by.x = "date", by.y = "date_time_hourly")
```


### Cleaning the data

```{r}
if (!file.exists(paste0(envrmt$path_model_training_data, "/df_clean_with_station.RDS"))) {
  # Removing NA's
  df_final_na <- df_final_merged[rowSums(is.na(df_final_merged[ , c(2:68,71:98)])) == 0, ]

  # Balancing the data so that from every station the same amount of data is used 
  downTrainDF <- downSample(x = df_final_na[, -70],y = as.factor(df_final_na$tree_id),yname = "tree_id")

  traintmp = downTrainDF
  # filter zero or near-zero values
  nzv = nearZeroVar(traintmp)
  if (length(nzv) > 0) traintmp = traintmp[, -nzv]
  
  # Removing rows with "inf" value
  traintmp <- traintmp[!is.infinite(rowSums(traintmp[,c(2:67,69:92)])),]
  
  # Remove temporary the non-predictor columns. 
  traintmp_rmv = traintmp[ , !(names(traintmp) %in% c("temp","date","tree_id"))]
  tDF = traintmp
  
  # filter correlations that are > cor_cutoff
  filt = findCorrelation(cor(traintmp_rmv, use = "complete"), cutoff = 0.9)
  traintmp_rmv = traintmp_rmv[,-filt]
  # re-add the necessary variables for model training
  traintmp_rmv$temp = tDF$temp
  traintmp_rmv$tree_id = tDF$tree_id
  traintmp_rmv$date = tDF$date
  # remove rows with NA
  traintmp_rmv = traintmp_rmv[complete.cases(traintmp_rmv) ,]

  # check manually if there are still NA values around
  summary(traintmp_rmv)
  sapply(traintmp_rmv, function(y) sum(length(which(is.na(y)))))
  
  df_final <- traintmp_rmv

  saveRDS(df_final,paste0(envrmt$path_model_training_data,"/df_clean_with_station.RDS"))

} else {
  
  df_final <- readRDS(paste0(envrmt$path_model_training_data, "/df_clean_with_station.RDS"))
}
```


### Split dataset into train and test data

```{r}
if (!file.exists(file.path(envrmt$path_model_training_data, "df_train_with_station.RDS"))) {
  
  # Set seed
  set.seed(11)
  
  # Split data into 80% for training and 20% for testing
  train_index <- caret::createDataPartition(df_final$temp, p = 0.8, list = FALSE)
  training <- df_final[train_index,]
  testing <- df_final[-train_index,]
  saveRDS(training, file.path(envrmt$path_model_training_data, "df_train_with_station.RDS"))
  saveRDS(testing, file.path(envrmt$path_model_training_data, "df_test_with_station.RDS"))
  
} else {
  
  training <- readRDS(file.path(envrmt$path_model_training_data, "df_train_with_station.RDS"))
  testing <- readRDS(file.path(envrmt$path_model_training_data, "df_test_with_station.RDS"))
}
```

```{r}
# Splitting the dataset from Reudenbach. 
#df_final_reudenbach <- readRDS(file.path(envrmt$path_processed, "trainDFmcClean.RDS"))


# Set seed
#set.seed(11)
  
# Split data into 80% for training and 20% for testing
#train_index <- caret::createDataPartition(df_final_reudenbach$temp, p = 0.8, list = FALSE)
#training <- df_final_reudenbach[train_index,]
#testing <- df_final_reudenbach[-train_index,]
```


### Create a random forest model using a Leave-Location-Out Cross-Validation (LLOCV) and Forward-Feature-Selection (FFS)

```{r}
# Function to calculate rmse with less code 
rmse <- function(predicted,observed,round = 2){
  return(round(sqrt(mean((predicted - observed)^2, na.rm = TRUE)), round))
}

# Function to remove stations, calculate a ranger model and calculate average rmse. More info in function description and return. 
model_results <- function(training_data,testing_data,nr_stations_out,first_seed,second_seeds,validation_runs,rounding = 2){
  #'@title model_results.
  #'@description Function calculates a ML ranger model while a definite amount of stations are removed. 
  #'@param training_data Training data for the ML model.  
  #'@param testing_data Testing data for the ML model.  
  #'@param nr_stations_out Number of stations that will randomly be removed. 
  #'@param first_seed One seed that is used for randomly removing tree stations.    
  #'@param second_seeds Needs to be the same number of seeds as "validation_runs". These seeds are used for different model runs. 
  #'@param validation_runs Number of runs the model will be excecuted for the same removed stations. In our case this number should usually be 3. 
  #'@param rounding Is standard "2". RMSE value results are rounded to this position after decimal point.
  #'@return Function returns a dataframe that gives the average rmse value for number of validation runs. It returns the RMSE of all stations, the stations removed, the stations still in and also the RMSE of each station. It also provides the used seeds so that they are reproducible and the number of stations left out and which these stations are. 

  # Selecting randomly tree stations
  tree_id <- unique(as.character(training_data$tree_id))
  set.seed(first_seed)
  stations_remove <- sample(tree_id,nr_stations_out) 


  # Removing the rows with the selected stations 
  for (k in seq(length(stations_remove))){
    training_data <- training_data[training_data$tree_id != stations_remove[k],]
    training_data <- droplevels(training_data)
  }

  # Dataframe erstellen indem die einzelnen Ergebnisse bei verschiedenen seeds gespeichert werden. 
  df_matrix <- matrix(ncol = 1, nrow = validation_runs)
  df_results <- data.frame(df_matrix)

  # Dann kann man eigentlich schon das Modell laufen lassen
  predictors <- training_data[,c(1:46)]
  
  response <- training_data[,"temp"]

  seeds = second_seeds

  # Itterieren ueber die verschiedenen seeds 
  for (x in 1:validation_runs){

    # Define parameters for the LLOCV. 
    llocv <- CreateSpacetimeFolds(training_data, spacevar = "tree_id",
                                  k = 10, class = "temp")

    # Control the parameters for later training
    # --> the folds of the LLOCV are passed as an index
    set.seed(seeds[x]) 
    ctrl <- trainControl(method = "cv", index = llocv$index,
                     savePredictions = TRUE, allowParallel = TRUE)

    # Control the parameters for model tuning
    # Tuneable parameter for random forests in the package 'ranger':
    # mtry = Number of variables to possibly split at in each node
    # splitrule = Splitting rule. For classification "gini", "extratrees" or "hellinger"
    # min.node.size = Minimal node size
    # https://www.rdocumentation.org/packages/ranger/versions/0.13.1/topics/ranger
    # http://topepo.github.io/caret/available-models.html
    tgrid <- expand.grid(
      mtry = c(5:10),#5:10
      splitrule = "extratrees",
      min.node.size = c(5,10,15)
    )
    # Noted from previous model runs that low "min.node.size" and high "mtry" values worked best. 

  
    set.seed(seeds[x])
  
    model <- train(predictors,
          response,
          method = "ranger",
          metric = "RMSE",
          num.trees = 100,
          tuneGrid = tgrid,
          trControl = ctrl,
          importance = 'permutation')


    df_results$run[x] <- x 
    df_results$first_seed[x] <- first_seed
    df_results$second_seed[x] <- seeds[x]
    
    # Deleting unneeded column
    if ("df_matrix" %in% colnames(df_results)){
      df_results$df_matrix <- NULL
    }

    # Prediction / Validierung
    predicted = stats::predict(object = model, newdata = testing_data)

    val_df = data.frame(ID = dplyr::pull(testing_data, "tree_id"),
                        Observed = dplyr::pull(testing_data, "temp"),
                        Predicted = predicted)


    df_results$amount_station_out[x] <- length(stations_remove)
    df_results$left_out[x] <- list(substr(stations_remove, 12, 13))

    df_results$rmse_gesamt[x] <- rmse(val_df$Predicted,val_df$Observed,rounding) 

    # Stationen die entfernt wurden zu neuen dataframe geben
    rmse_removed_stations <- val_df[val_df$ID %in% stations_remove, ] 
    df_results$rmse_left_out[x] <- rmse(rmse_removed_stations$Predicted,rmse_removed_stations$Observed,rounding) 

    # RMSE von Stationen die drinnen sind. 
    rmse_stations_in <- val_df[!val_df$ID %in% stations_remove, ] 
    df_results$rmse_station_in[x] <- rmse(rmse_stations_in$Predicted,rmse_stations_in$Observed,rounding)

    df_rmse_stat <- data.frame(station = unique(val_df$ID))

    for (n in 1:length(unique(val_df$ID))){
      rmse_removed_stations_si <- val_df[val_df$ID %in% df_rmse_stat$station[n], ]
      stationen_rmse <- rmse(rmse_removed_stations_si$Predicted,rmse_removed_stations_si$Observed)
      df_rmse_stat$rmse[n] <- stationen_rmse
    }

    tabs_zsm <- xtabs(rmse~station, df_rmse_stat)

    # Zu einem dataframe hinzufuegen
    df_table <- as.data.frame.matrix(t(tabs_zsm))

    # Verbinden mit dem dataframe zuvor

    if (x == 1){
      df_results <- cbind(df_results, df_table)
    } else if (x > 1){
      # Adding the rmse values of each station to dataframe
      for (i in 1:18){
        rmse_each_station <- df_table[[i]]
        df_results[x,i+8] <- rmse_each_station
      }
    }

  }

  df_mean_results <- df_results[1,c(4:5)]
  df_mean_results$first_seed <- df_results$first_seed[1]
  df_mean_results$second_seeds <- list(df_results$second_seed)
  df_mean_results[,5:25] <- sapply(df_results[,c(6:26)],FUN=mean)
  names(df_mean_results)[5:25] <- names(df_results[c(6:26)])

  return(df_mean_results)

}
```

```{r}
# Testing the function.
result <- model_results(training,testing,2,98,c(64,13,649),3,2)

# This result will be saved in the "prediction" folder as "model_results.RDS" and each new model run can be added to this dataframe. 

if (!file.exists(paste0(envrmt$path_prediction, "/model_results.RDS"))) {
  
  saveRDS(result, paste0(envrmt$path_prediction, "/model_results.RDS"))

} else {
  
  df_full_results <- readRDS(paste0(envrmt$path_prediction, "/model_results.RDS"))

}
# Now the dataframe where all the results are stored is loaded and by calling the "model_results" function and afterwards using the "rbind" function as demonstrated below you can just add all the results into one dataframe. But don't forget to save the dataframe after each different function run. 

# Using the "model_results" function
result <- model_results(training,testing,2,42,c(29,18,98),3,2)
# Add the new results to the exisiting dataframe.  
df_full_results <- rbind(df_full_results, result)# "results" is the output of the "model_results" function and "df_full_results" the loaded result dataframe. 

# Save the updated results
saveRDS(df_full_results, paste0(envrmt$path_prediction, "/model_results.RDS"))
```






























